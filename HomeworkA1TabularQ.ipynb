{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from really import SampleManager\n",
    "from gridworlds import GridWorld\n",
    "#%%time\n",
    "import random\n",
    "import os\n",
    "#from really import SampleManager  # important !!\n",
    "from really.utils import (\n",
    "    dict_to_dict_of_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ(object):\n",
    "\n",
    "    def __init__(self, h, w, action_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        #print(h,w)\n",
    "        self.q_table = np.zeros((h, w, 4))\n",
    "\n",
    "\n",
    "        #self.q_table.fill(0)\n",
    "\n",
    "\n",
    "        #print(self.q_table)\n",
    "        #print(self.q_table[2, 2])\n",
    "        #q_table[0][3]\n",
    "\n",
    "        ## # TODO:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, state):\n",
    "        ## # TODO:\n",
    "\n",
    "        output = {}\n",
    "        #print(state)\n",
    "        #print(state[0][0], state[0][1])\n",
    "        #print(self.q_table[2, 2])\n",
    "        a = int(state[0][0])\n",
    "        b = int(state[0][1])\n",
    "        #print(a, b)\n",
    "        #print(np.asmatrix(self.q_table[a, b]))\n",
    "\n",
    "        output[\"q_values\"] = np.asmatrix(self.q_table[a, b]) # achtung vllt []\n",
    "\n",
    "\n",
    "        #output[\"q_values\"] = np.random.normal(size=(1, self.action_space))\n",
    "        #print(output[\"q_values\"])\n",
    "        #print(np.random.normal(size=(1, self.action_space)))\n",
    "\n",
    "        return output\n",
    "\n",
    "    # # TODO:\n",
    "    def get_weights(self):\n",
    "\n",
    "        #output =\n",
    "\n",
    "        return self.q_table\n",
    "\n",
    "\n",
    "    def set_weights(self, q_vals):\n",
    "\n",
    "        print(q_vals)\n",
    "        self.q_table = q_vals\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 19:40:00,070\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.178.21',\n",
       " 'raylet_ip_address': '192.168.178.21',\n",
       " 'redis_address': '192.168.178.21:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-02-25_19-39-59_490613_70820/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-02-25_19-39-59_490613_70820/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-02-25_19-39-59_490613_70820',\n",
       " 'metrics_export_port': 63681,\n",
       " 'node_id': '62b239937b20fba2e2b697f96a5428452672d34817dca2f83a67759e'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(log_to_driver=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, storing results of 1 runners\n"
     ]
    }
   ],
   "source": [
    "action_dict = {0: \"UP\", 1: \"RIGHT\", 2: \"DOWN\", 3: \"LEFT\"}\n",
    "\n",
    "env_kwargs = {\n",
    "    \"height\": 3,\n",
    "    \"width\": 4,\n",
    "    \"action_dict\": action_dict,\n",
    "    \"start_position\": (2, 0),\n",
    "    \"reward_position\": (0, 3),\n",
    "}\n",
    "\n",
    "# you can also create your environment like this after installation: env = gym.make('gridworld-v0')\n",
    "env = GridWorld(**env_kwargs)\n",
    "\n",
    "model_kwargs = {\"h\": env.height, \"w\": env.width, \"action_space\": 4}\n",
    "\n",
    "kwargs = {\n",
    "    \"model\": TabularQ,\n",
    "    \"environment\": GridWorld,\n",
    "    \"num_parallel\": 2,\n",
    "    \"total_steps\": 5,\n",
    "    \"model_kwargs\": model_kwargs\n",
    "\n",
    "    # and more\n",
    "}\n",
    "\n",
    "# initilize\n",
    "manager = SampleManager(**kwargs)\n",
    "#print(manager.sample(10))\n",
    "\n",
    "manager.get_data(do_print=True)\n",
    "\n",
    "# where to save your results to: create this directory in advance!\n",
    "saving_path = os.getcwd() + \"../Homework/A1/progress_test\"\n",
    "\n",
    "buffer_size = 5000\n",
    "test_steps = 1000\n",
    "epochs = 5\n",
    "sample_size = 1000\n",
    "optim_batch_size = 8\n",
    "saving_after = 5\n",
    "alpha = 0.2\n",
    "gamma = 0.85\n",
    "\n",
    "# keys for replay buffer -> what you will need for optimization\n",
    "optim_keys = [\"state\", \"action\", \"reward\", \"state_new\", \"not_done\"]\n",
    "\n",
    "# initialize buffer\n",
    "manager.initilize_buffer(buffer_size, optim_keys)\n",
    "\n",
    "# initilize progress aggregator\n",
    "manager.initialize_aggregator(\n",
    "    path=saving_path, saving_after=5, aggregator_keys=[\"loss\", \"time_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test before training: \n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "Episodes finished after a mean of 42.33 timesteps\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# initial testing:\n",
    "print(\"test before training: \")\n",
    "manager.test(test_steps, do_print=True)\n",
    "\n",
    "# get initial agent\n",
    "agent = manager.get_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting experience..\n",
      "collected data for: dict_keys(['state', 'action', 'reward', 'state_new', 'not_done'])\n",
      "> \u001b[0;32m<ipython-input-12-58d63b21dbcb>\u001b[0m(20)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     18 \u001b[0;31m                     for old_value, r, next_max in zip(old_value_batch, r_batch, next_max_batch)]\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 20 \u001b[0;31m        \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m            \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(s_batch)\n",
      "tf.Tensor(\n",
      "[[2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 1]\n",
      " [1 0]], shape=(8, 2), dtype=int64)\n",
      "ipdb> np.concatenate\n",
      "<function concatenate at 0x7ff74c289430>\n",
      "ipdb> np.concatenate(s_batch, a, axis=1)\n",
      "*** TypeError: concatenate() got multiple values for argument 'axis'\n",
      "ipdb> np.concatenate((s_batch, a_batch), axis=1)\n",
      "*** ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n",
      "ipdb> np.concatenate((s_batch, a_batch), axis=1)\n",
      "*** ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n",
      "ipdb> np.concatenate((s_batch, a_batch), axis=0)\n",
      "*** ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    print(\"collecting experience..\")\n",
    "    data = manager.get_data()\n",
    "    manager.store_in_buffer(data)\n",
    "\n",
    "    # sample data to optimize on from buffer\n",
    "    sample_dict = manager.sample(sample_size)\n",
    "    print(f\"collected data for: {sample_dict.keys()}\")\n",
    "    # create and batch tf datasets\n",
    "    data_dict = dict_to_dict_of_datasets(sample_dict, batch_size=optim_batch_size)\n",
    "    state, next_state, action, reward = data_dict['state'], data_dict['state_new'], data_dict['action'], data_dict['reward']  \n",
    "    q_table = agent.get_weights().copy()\n",
    "    for s_batch, s_next_batch, a_batch, r_batch in zip(state, next_state, action, reward):\n",
    "        \n",
    "        old_value_batch = [q_table[s[0], s[1], a] for s, a in zip(s_batch, a_batch)]\n",
    "        next_max_batch = [np.max(q_table[s_next[0], s_next[1]]) for s_next in s_next_batch]\n",
    "        new_values = [old_value + alpha * ((r.numpy()) + gamma * next_max - old_value)\n",
    "                     for old_value, r, next_max in zip(old_value_batch, r_batch, next_max_batch)]\n",
    "        breakpoint()\n",
    "        q_table[s[0][0], s[0][1], a[0]] = new_value\n",
    "        if r.numpy()[0] != 0:\n",
    "            pass\n",
    "            #breakpoint()\n",
    "    # set new weights\n",
    "    #breakpoint()\n",
    "    manager.set_agent(q_table)\n",
    "    # get new weights\n",
    "    agent = manager.get_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 4.43705312  5.2200625   5.2200625   4.43705312]\n",
      "  [ 5.2200625   6.14125     5.2200625   4.43705312]\n",
      "  [ 6.14125     7.225       7.225       5.2200625 ]\n",
      "  [ 7.225       7.225       8.5         6.14125   ]]\n",
      "\n",
      " [[ 4.43705312  5.2200625   6.14125     5.2200625 ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 6.14125     8.5         8.5         7.225     ]\n",
      "  [ 7.2236474   8.4999994  10.          7.225     ]]\n",
      "\n",
      " [[ 5.2200625   7.225       6.14125     6.14125   ]\n",
      "  [ 7.225       8.5         7.225       6.14125   ]\n",
      "  [ 7.225      10.          8.5         7.225     ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "Episodes finished after a mean of 4.0 timesteps\n",
      "Episodes finished after a mean of 2.0 accumulated reward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.test(\n",
    "       max_steps=100,\n",
    "        test_episodes=10,\n",
    "        render=True,\n",
    "        do_print=True,\n",
    "        evaluation_measure=\"time_and_reward\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
