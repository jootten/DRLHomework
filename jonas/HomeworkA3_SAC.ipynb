{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "civilian-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, logging, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import ray, gym\n",
    "\n",
    "from tensorflow_probability.python.distributions import MultivariateNormalDiag, TransformedDistribution, Categorical\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opened-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "logging.disable(logging.WARNING)\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "individual-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"FIFO experience replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, obs_shape, action_shape):\n",
    "        self.size = size        \n",
    "        self.total = 0\n",
    "        self.data = {\n",
    "            'state': np.zeros((size, *obs_shape), dtype=np.float64),\n",
    "            'action': np.zeros((size, *action_shape), dtype=np.float64),\n",
    "            'reward': np.zeros((size, 1), dtype=np.float64),\n",
    "            'next_state': np.zeros((size, *obs_shape), dtype=np.float64),\n",
    "            'done': np.zeros((size, 1), dtype=np.float64)\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def num_stored(self):\n",
    "        return self.total if self.total < self.size else self.size\n",
    "    \n",
    "    @property\n",
    "    def cur_idx(self):\n",
    "        return self.num_stored % self.size\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.total += 1\n",
    "        self.data['state'][self.cur_idx] = state\n",
    "        self.data['action'][self.cur_idx] = action\n",
    "        self.data['reward'][self.cur_idx] = reward\n",
    "        self.data['next_state'][self.cur_idx] = next_state\n",
    "        self.data['done'][self.cur_idx] = done\n",
    "        \n",
    "    def sample_list_of_datasets(self, \n",
    "                                batch_size, \n",
    "                                dataset_size,\n",
    "                                map_func=None,\n",
    "                                prioritized_sampling=False,\n",
    "                                prio_sampling_degree=0.2):\n",
    "        if prioritized_sampling:\n",
    "            abs_reward = np.abs(self.data['reward'][:self.num_stored]).squeeze()\n",
    "            probs =  abs_reward**prio_sampling_degree / np.sum(abs_reward**prio_sampling_degree)\n",
    "            sample_dist = Categorical(probs=probs)\n",
    "            indices = sample_dist.sample((dataset_size,)).numpy()\n",
    "            entropy_diff = Categorical(np.repeat(1/self.num_stored, self.num_stored)).entropy() - sample_dist.entropy()\n",
    "            tf.summary.scalar('metrics/prio_sample_dist_entropy_diff', \n",
    "                              entropy_diff, \n",
    "                              description=\"Difference in entropy between uniform and prioritized sampling distribution.\")\n",
    "        else:\n",
    "            indices = np.random.randint(0, self.num_stored, dataset_size)\n",
    "\n",
    "        dataset_list = []\n",
    "        for k in self.data.keys():\n",
    "            dataset_list.append(tf.data.Dataset.from_tensor_slices(\n",
    "                    tf.convert_to_tensor(self.data[k][indices])\n",
    "                ).batch(batch_size, drop_remainder=True).map(map_func))\n",
    "        return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "administrative-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use clipped log sigma for stable training\n",
    "clip_log_sigma = (-20., 2.)\n",
    "\n",
    "class ActorNetwork(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=None,\n",
    "                 action_space=2, \n",
    "                 normalize_mean=None,\n",
    "                 normalize_sd=None):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        if normalize_mean is not None:\n",
    "            assert normalize_sd is not None\n",
    "            assert normalize_sd.shape == input_shape\n",
    "            assert normalize_mean.shape == input_shape\n",
    "        self.normalize_mean = normalize_mean\n",
    "        self.normalize_sd = normalize_sd\n",
    "        \n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, input_shape=input_shape,\n",
    "                                  activation='relu', kernel_regularizer='l2'),\n",
    "            tf.keras.layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "        ])\n",
    "        self.fc_mu = tf.keras.layers.Dense(action_space, use_bias=False)\n",
    "        self.fc_log_sigma = tf.keras.layers.Dense(action_space, use_bias=False)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if self.normalize_mean is not None:\n",
    "            x = (x - self.normalize_mean) / self.normalize_sd\n",
    "            \n",
    "        x = self.mlp(x)\n",
    "        output = {}\n",
    "        output[\"mu\"] = self.fc_mu(x)\n",
    "        log_sigma = tf.clip_by_value(self.fc_log_sigma(x), *clip_log_sigma)\n",
    "        output[\"sigma\"] = tf.math.exp(log_sigma)\n",
    "        return output\n",
    "    \n",
    "    def act(self, x, return_log_prob=False, random_agent=False):\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis,]\n",
    "        net_out = self.call(x)\n",
    "        output = {}\n",
    "        if not random_agent:\n",
    "            mus, sigmas = net_out[\"mu\"], net_out[\"sigma\"]\n",
    "\n",
    "            action_distribution = TransformedDistribution(\n",
    "                MultivariateNormalDiag(loc=mus, scale_diag=sigmas),\n",
    "                tfp.bijectors.Tanh())\n",
    "\n",
    "            # bounded by [-1, 1]\n",
    "            action = action_distribution.sample()\n",
    "            output[\"action\"] = tf.squeeze(action)\n",
    "\n",
    "            if return_log_prob:\n",
    "                output[\"log_probability\"] = action_distribution.log_prob(action)[:,tf.newaxis]\n",
    "        else:\n",
    "            output[\"action\"] = np.random.uniform(-1., 1., (self.action_space,))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "waiting-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_mlp(units=(256, 256), activation='relu', kernel_regularizer=None):\n",
    "    layers = [tf.keras.layers.Dense(u, activation=activation, kernel_regularizer=kernel_regularizer)\n",
    "              for u in units]\n",
    "    layers.append(tf.keras.layers.Dense(1))\n",
    "    return tf.keras.Sequential(layers)\n",
    "        \n",
    "class DoubleQNetwork(Model):\n",
    "    def __init__(self,\n",
    "                 action_space=2,\n",
    "                 normalize_mean=None,\n",
    "                 normalize_sd=None):\n",
    "        \n",
    "        if normalize_mean is not None:\n",
    "            assert normalize_sd is not None\n",
    "        self.normalize_mean = normalize_mean\n",
    "        self.normalize_sd = normalize_sd\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        super(DoubleQNetwork, self).__init__()\n",
    "        self.qnet_1 = get_q_mlp()\n",
    "        self.qnet_2 = get_q_mlp()\n",
    "        \n",
    "    def call(self, state, action):\n",
    "        x = tf.concat((state, action), axis=1)\n",
    "\n",
    "        if self.normalize_mean is not None:\n",
    "            x = (x - self.normalize_mean) / self.normalize_sd\n",
    "            \n",
    "        return {\"q1\": self.qnet_1(x), \n",
    "                \"q2\": self.qnet_2(x)}\n",
    "    \n",
    "    def update_normalization(self, normalize_mean, normalize_sd):\n",
    "        self.normalize_mean = normalize_mean\n",
    "        self.normalize_sd = normalize_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "essential-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1., -1.], dtype=float32), array([1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = \"LunarLanderContinuous-v2\"\n",
    "env = gym.make(env_id)\n",
    "env.action_space.low, env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "knowing-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_every_n_step = 1\n",
    "\n",
    "buffer_size = 100000\n",
    "warmup_steps = 4000\n",
    "total_timesteps = 500000\n",
    "epoch_length = 1000\n",
    "epochs = total_timesteps // epoch_length\n",
    "log_every_n_step = 100\n",
    "test_every = 1\n",
    "\n",
    "\n",
    "optim_batch_size = 256\n",
    "# lock ratio between env steps and gradient updates to 1\n",
    "sample_size = optim_batch_size * train_every_n_step\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "# entropy coefficient\n",
    "alpha = 0.6\n",
    "# polyak averaging\n",
    "polyak = 0.995\n",
    "# exponential moving average\n",
    "alpha_exp_avg = 0.005\n",
    "# whether to oversample SARS pairs with extreme (sparse) rewards\n",
    "prioritized_sampling = True\n",
    "prio_sampling_degree = 0.2\n",
    "# whether to normalize inputs (zero mean, one std)\n",
    "normalize_inputs = False\n",
    "\n",
    "model_kwargs = {\n",
    "    \"input_shape\": env.observation_space.shape,\n",
    "    \"action_space\": env.action_space.shape[0],\n",
    "    \"normalize_mean\": np.zeros(env.observation_space.shape),\n",
    "    \"normalize_sd\": np.ones(env.observation_space.shape)\n",
    "}\n",
    "\n",
    "qnet_kwargs = {\n",
    "    \"action_space\": env.action_space.shape[0],\n",
    "    \"normalize_mean\": np.zeros((env.observation_space.shape[0] + env.action_space.shape[0],)),\n",
    "    \"normalize_sd\": np.ones((env.observation_space.shape[0] + env.action_space.shape[0],))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bigger-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put update into function to make loop less cluttered\n",
    "def update_normalization(moving_mean, moving_std):\n",
    "    actor.normalize_mean = moving_mean[:env.observation_space.shape[0]]\n",
    "    actor.normalize_sd = moving_std[:env.observation_space.shape[0]]\n",
    "    update_DoubleQNetwork.update_normalization(moving_mean, moving_std)\n",
    "    target_DoubleQNetwork.update_normalization(moving_mean, moving_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pressed-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(env, actor, buffer, steps):\n",
    "    state = env.reset()\n",
    "    for _ in range(steps):\n",
    "        action = actor.act(state, random_agent=True)['action']\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        buffer.store(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "    return env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nasty-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env_id, actor, test_episodes=10, render=False):\n",
    "    test_env = gym.make(env_id)\n",
    "    returns = []\n",
    "    for _ in range(test_episodes):\n",
    "        test_return = 0\n",
    "        state = test_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = actor.act(state)['action']\n",
    "            state, reward, done, info = test_env.step(action)\n",
    "            test_return += reward\n",
    "        returns.append(test_return)\n",
    "    test_env.close()\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mysterious-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO normalize q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "marked-fortune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update step: 1000\n",
      "epoch ::: 0   episode ::: 42.0   update step ::: 1000   time elapsed ::: 00:03:03\n",
      "critic loss avg ::: 184.11   min ::: 10.32    max ::: 1006.13\n",
      "actor loss avg ::: 5.06   min ::: -0.65   max ::: 11.68\n",
      "q-vals avg ::: -5.55   min ::: -11.95   max ::: -0.14\n",
      "log_prob avg ::: -0.81   min ::: -1.39   max ::: -0.21\n",
      "env return avg ::: -271.6   buffer size ::: 5000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 2000\n",
      "epoch ::: 1   episode ::: 48.0   update step ::: 2000   time elapsed ::: 00:06:05\n",
      "critic loss avg ::: 70.97   min ::: 7.86    max ::: 265.54\n",
      "actor loss avg ::: 9.42   min ::: 4.19   max ::: 15.71\n",
      "q-vals avg ::: -9.58   min ::: -15.79   max ::: -4.5\n",
      "log_prob avg ::: -0.27   min ::: -0.7   max ::: 0.15\n",
      "env return avg ::: -267.4   buffer size ::: 6000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 3000\n",
      "epoch ::: 2   episode ::: 55.0   update step ::: 3000   time elapsed ::: 00:09:07\n",
      "critic loss avg ::: 49.59   min ::: 9.06    max ::: 150.71\n",
      "actor loss avg ::: 12.8   min ::: 6.21   max ::: 22.25\n",
      "q-vals avg ::: -12.87   min ::: -22.25   max ::: -6.34\n",
      "log_prob avg ::: -0.11   min ::: -0.41   max ::: 0.25\n",
      "env return avg ::: -208.92   buffer size ::: 7000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 4000\n",
      "epoch ::: 3   episode ::: 60.0   update step ::: 4000   time elapsed ::: 00:12:22\n",
      "critic loss avg ::: 41.2   min ::: 7.95    max ::: 130.92\n",
      "actor loss avg ::: 15.82   min ::: 9.08   max ::: 23.75\n",
      "q-vals avg ::: -15.81   min ::: -23.64   max ::: -9.11\n",
      "log_prob avg ::: 0.02   min ::: -0.37   max ::: 0.39\n",
      "env return avg ::: -80.39   buffer size ::: 8000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 5000\n",
      "epoch ::: 4   episode ::: 63.0   update step ::: 5000   time elapsed ::: 00:15:44\n",
      "critic loss avg ::: 35.09   min ::: 7.75    max ::: 110.52\n",
      "actor loss avg ::: 16.87   min ::: 8.36   max ::: 24.99\n",
      "q-vals avg ::: -16.86   min ::: -24.94   max ::: -8.41\n",
      "log_prob avg ::: 0.02   min ::: -0.27   max ::: 0.36\n",
      "env return avg ::: -45.14   buffer size ::: 9000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 6000\n",
      "epoch ::: 5   episode ::: 66.0   update step ::: 6000   time elapsed ::: 00:19:27\n",
      "critic loss avg ::: 27.83   min ::: 7.17    max ::: 75.95\n",
      "actor loss avg ::: 15.6   min ::: 7.37   max ::: 25.0\n",
      "q-vals avg ::: -15.6   min ::: -24.95   max ::: -7.47\n",
      "log_prob avg ::: 0.0   min ::: -0.27   max ::: 0.27\n",
      "env return avg ::: -6.99   buffer size ::: 10000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 7000\n",
      "epoch ::: 6   episode ::: 67.0   update step ::: 7000   time elapsed ::: 00:23:14\n",
      "critic loss avg ::: 22.05   min ::: 5.38    max ::: 61.69\n",
      "actor loss avg ::: 13.08   min ::: 3.77   max ::: 23.87\n",
      "q-vals avg ::: -13.1   min ::: -23.9   max ::: -3.78\n",
      "log_prob avg ::: -0.04   min ::: -0.34   max ::: 0.29\n",
      "env return avg ::: 35.43   buffer size ::: 11000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 8000\n",
      "epoch ::: 7   episode ::: 69.0   update step ::: 8000   time elapsed ::: 00:27:02\n",
      "critic loss avg ::: 17.69   min ::: 5.35    max ::: 50.44\n",
      "actor loss avg ::: 11.25   min ::: 0.73   max ::: 21.88\n",
      "q-vals avg ::: -11.29   min ::: -21.85   max ::: -0.82\n",
      "log_prob avg ::: -0.06   min ::: -0.32   max ::: 0.25\n",
      "env return avg ::: 8.53   buffer size ::: 12000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 9000\n",
      "epoch ::: 8   episode ::: 71.0   update step ::: 9000   time elapsed ::: 00:31:02\n",
      "critic loss avg ::: 25.07   min ::: 4.4    max ::: 327.15\n",
      "actor loss avg ::: 10.21   min ::: 2.33   max ::: 20.53\n",
      "q-vals avg ::: -10.27   min ::: -20.55   max ::: -2.4\n",
      "log_prob avg ::: -0.09   min ::: -0.5   max ::: 0.27\n",
      "env return avg ::: 29.73   buffer size ::: 13000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 10000\n",
      "epoch ::: 9   episode ::: 72.0   update step ::: 10000   time elapsed ::: 00:35:02\n",
      "critic loss avg ::: 32.23   min ::: 5.3    max ::: 551.19\n",
      "actor loss avg ::: 10.09   min ::: -0.52   max ::: 20.33\n",
      "q-vals avg ::: -10.16   min ::: -20.41   max ::: 0.41\n",
      "log_prob avg ::: -0.11   min ::: -0.39   max ::: 0.16\n",
      "env return avg ::: -10.88   buffer size ::: 14000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 11000\n",
      "epoch ::: 10   episode ::: 74.0   update step ::: 11000   time elapsed ::: 00:39:12\n",
      "critic loss avg ::: 41.11   min ::: 4.95    max ::: 466.45\n",
      "actor loss avg ::: 9.2   min ::: 0.53   max ::: 20.17\n",
      "q-vals avg ::: -9.28   min ::: -20.18   max ::: -0.7\n",
      "log_prob avg ::: -0.12   min ::: -0.43   max ::: 0.16\n",
      "env return avg ::: 77.26   buffer size ::: 15000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 12000\n",
      "epoch ::: 11   episode ::: 75.0   update step ::: 12000   time elapsed ::: 00:43:19\n",
      "critic loss avg ::: 35.82   min ::: 5.79    max ::: 353.79\n",
      "actor loss avg ::: 7.96   min ::: -2.2   max ::: 20.62\n",
      "q-vals avg ::: -8.06   min ::: -20.59   max ::: 1.99\n",
      "log_prob avg ::: -0.15   min ::: -0.45   max ::: 0.13\n",
      "env return avg ::: 153.69   buffer size ::: 16000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 13000\n",
      "epoch ::: 12   episode ::: 76.0   update step ::: 13000   time elapsed ::: 00:47:23\n",
      "critic loss avg ::: 28.58   min ::: 3.99    max ::: 292.47\n",
      "actor loss avg ::: 6.48   min ::: -2.08   max ::: 15.72\n",
      "q-vals avg ::: -6.59   min ::: -15.77   max ::: 1.92\n",
      "log_prob avg ::: -0.18   min ::: -0.46   max ::: 0.15\n",
      "env return avg ::: 116.11   buffer size ::: 17000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 14000\n",
      "epoch ::: 13   episode ::: 78.0   update step ::: 14000   time elapsed ::: 00:51:17\n",
      "critic loss avg ::: 32.69   min ::: 5.26    max ::: 359.76\n",
      "actor loss avg ::: 5.26   min ::: -3.7   max ::: 15.35\n",
      "q-vals avg ::: -5.39   min ::: -15.39   max ::: 3.55\n",
      "log_prob avg ::: -0.21   min ::: -0.49   max ::: 0.1\n",
      "env return avg ::: 107.88   buffer size ::: 18000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 15000\n",
      "epoch ::: 14   episode ::: 80.0   update step ::: 15000   time elapsed ::: 00:55:53\n",
      "critic loss avg ::: 35.62   min ::: 4.25    max ::: 286.84\n",
      "actor loss avg ::: 3.95   min ::: -6.88   max ::: 13.58\n",
      "q-vals avg ::: -4.08   min ::: -13.65   max ::: 6.64\n",
      "log_prob avg ::: -0.22   min ::: -0.49   max ::: 0.11\n",
      "env return avg ::: 86.5   buffer size ::: 19000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 16000\n",
      "epoch ::: 15   episode ::: 81.0   update step ::: 16000   time elapsed ::: 00:59:58\n",
      "critic loss avg ::: 33.62   min ::: 5.21    max ::: 194.07\n",
      "actor loss avg ::: 2.4   min ::: -6.91   max ::: 12.85\n",
      "q-vals avg ::: -2.55   min ::: -12.92   max ::: 6.8\n",
      "log_prob avg ::: -0.25   min ::: -0.54   max ::: 0.05\n",
      "env return avg ::: 127.0   buffer size ::: 20000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 17000\n",
      "epoch ::: 16   episode ::: 82.0   update step ::: 17000   time elapsed ::: 01:04:20\n",
      "critic loss avg ::: 31.89   min ::: 4.69    max ::: 169.31\n",
      "actor loss avg ::: 0.55   min ::: -7.92   max ::: 9.11\n",
      "q-vals avg ::: -0.72   min ::: -9.23   max ::: 7.75\n",
      "log_prob avg ::: -0.28   min ::: -0.56   max ::: 0.01\n",
      "env return avg ::: 106.51   buffer size ::: 21000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 18000\n",
      "epoch ::: 17   episode ::: 83.0   update step ::: 18000   time elapsed ::: 01:07:56\n",
      "critic loss avg ::: 30.92   min ::: 4.23    max ::: 165.11\n",
      "actor loss avg ::: -1.22   min ::: -9.88   max ::: 8.36\n",
      "q-vals avg ::: 1.05   min ::: -8.52   max ::: 9.64\n",
      "log_prob avg ::: -0.29   min ::: -0.59   max ::: -0.04\n",
      "env return avg ::: 134.74   buffer size ::: 22000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 19000\n",
      "epoch ::: 18   episode ::: 87.0   update step ::: 19000   time elapsed ::: 01:12:27\n",
      "critic loss avg ::: 29.95   min ::: 4.42    max ::: 144.16\n",
      "actor loss avg ::: -2.66   min ::: -9.93   max ::: 9.07\n",
      "q-vals avg ::: 2.48   min ::: -9.23   max ::: 9.72\n",
      "log_prob avg ::: -0.31   min ::: -0.55   max ::: 0.0\n",
      "env return avg ::: 54.23   buffer size ::: 23000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 20000\n",
      "epoch ::: 19   episode ::: 88.0   update step ::: 20000   time elapsed ::: 01:17:13\n",
      "critic loss avg ::: 31.66   min ::: 4.67    max ::: 162.93\n",
      "actor loss avg ::: -4.15   min ::: -11.82   max ::: 5.48\n",
      "q-vals avg ::: 3.96   min ::: -5.65   max ::: 11.54\n",
      "log_prob avg ::: -0.32   min ::: -0.58   max ::: -0.0\n",
      "env return avg ::: -16.34   buffer size ::: 24000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 21000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ::: 20   episode ::: 90.0   update step ::: 21000   time elapsed ::: 01:21:13\n",
      "critic loss avg ::: 32.21   min ::: 5.65    max ::: 184.07\n",
      "actor loss avg ::: -5.55   min ::: -14.79   max ::: 3.74\n",
      "q-vals avg ::: 5.36   min ::: -3.88   max ::: 14.51\n",
      "log_prob avg ::: -0.32   min ::: -0.62   max ::: -0.01\n",
      "env return avg ::: 13.88   buffer size ::: 25000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Early stop! Resetting...\n",
      "Update step: 1000\n",
      "epoch ::: 0   episode ::: 44.0   update step ::: 1000   time elapsed ::: 00:03:05\n",
      "critic loss avg ::: 206.42   min ::: 13.11    max ::: 981.42\n",
      "actor loss avg ::: 5.8   min ::: -0.58   max ::: 11.25\n",
      "q-vals avg ::: -6.31   min ::: -11.53   max ::: -0.2\n",
      "log_prob avg ::: -0.85   min ::: -1.38   max ::: -0.27\n",
      "env return avg ::: -232.67   buffer size ::: 5000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 2000\n",
      "epoch ::: 1   episode ::: 50.0   update step ::: 2000   time elapsed ::: 00:06:44\n",
      "critic loss avg ::: 85.42   min ::: 10.45    max ::: 242.88\n",
      "actor loss avg ::: 10.67   min ::: 5.93   max ::: 16.13\n",
      "q-vals avg ::: -10.89   min ::: -16.29   max ::: -6.18\n",
      "log_prob avg ::: -0.37   min ::: -0.68   max ::: -0.04\n",
      "env return avg ::: -112.66   buffer size ::: 6000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 3000\n",
      "epoch ::: 2   episode ::: 52.0   update step ::: 3000   time elapsed ::: 00:11:11\n",
      "critic loss avg ::: 64.89   min ::: 10.62    max ::: 179.84\n",
      "actor loss avg ::: 11.68   min ::: 5.52   max ::: 17.17\n",
      "q-vals avg ::: -11.82   min ::: -17.33   max ::: -5.49\n",
      "log_prob avg ::: -0.24   min ::: -0.54   max ::: 0.09\n",
      "env return avg ::: -89.52   buffer size ::: 7000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 4000\n",
      "epoch ::: 3   episode ::: 53.0   update step ::: 4000   time elapsed ::: 00:15:36\n",
      "critic loss avg ::: 49.02   min ::: 8.34    max ::: 140.01\n",
      "actor loss avg ::: 10.88   min ::: 4.95   max ::: 17.35\n",
      "q-vals avg ::: -10.97   min ::: -17.42   max ::: -5.11\n",
      "log_prob avg ::: -0.14   min ::: -0.48   max ::: 0.16\n",
      "env return avg ::: -49.45   buffer size ::: 8000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 5000\n",
      "epoch ::: 4   episode ::: 55.0   update step ::: 5000   time elapsed ::: 00:19:44\n",
      "critic loss avg ::: 40.7   min ::: 6.2    max ::: 144.03\n",
      "actor loss avg ::: 9.07   min ::: 1.86   max ::: 17.45\n",
      "q-vals avg ::: -9.11   min ::: -17.4   max ::: -2.04\n",
      "log_prob avg ::: -0.07   min ::: -0.42   max ::: 0.2\n",
      "env return avg ::: -62.81   buffer size ::: 9000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 6000\n",
      "epoch ::: 5   episode ::: 56.0   update step ::: 6000   time elapsed ::: 00:23:41\n",
      "critic loss avg ::: 32.15   min ::: 6.4    max ::: 118.46\n",
      "actor loss avg ::: 7.38   min ::: -0.52   max ::: 15.81\n",
      "q-vals avg ::: -7.4   min ::: -15.71   max ::: 0.4\n",
      "log_prob avg ::: -0.03   min ::: -0.38   max ::: 0.28\n",
      "env return avg ::: 10.95   buffer size ::: 10000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 7000\n",
      "epoch ::: 6   episode ::: 59.0   update step ::: 7000   time elapsed ::: 00:27:24\n",
      "critic loss avg ::: 43.62   min ::: 5.53    max ::: 495.15\n",
      "actor loss avg ::: 6.91   min ::: -0.9   max ::: 13.99\n",
      "q-vals avg ::: -6.92   min ::: -14.03   max ::: 0.87\n",
      "log_prob avg ::: -0.01   min ::: -0.35   max ::: 0.34\n",
      "env return avg ::: -54.8   buffer size ::: 11000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 8000\n",
      "epoch ::: 7   episode ::: 60.0   update step ::: 8000   time elapsed ::: 00:30:57\n",
      "critic loss avg ::: 42.22   min ::: 3.87    max ::: 478.25\n",
      "actor loss avg ::: 5.86   min ::: -3.53   max ::: 13.72\n",
      "q-vals avg ::: -5.87   min ::: -13.7   max ::: 3.4\n",
      "log_prob avg ::: -0.02   min ::: -0.4   max ::: 0.32\n",
      "env return avg ::: -34.29   buffer size ::: 12000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 9000\n",
      "epoch ::: 8   episode ::: 61.0   update step ::: 9000   time elapsed ::: 00:35:17\n",
      "critic loss avg ::: 53.54   min ::: 5.67    max ::: 437.12\n",
      "actor loss avg ::: 5.12   min ::: -3.06   max ::: 15.56\n",
      "q-vals avg ::: -5.15   min ::: -15.51   max ::: 3.0\n",
      "log_prob avg ::: -0.04   min ::: -0.32   max ::: 0.35\n",
      "env return avg ::: -78.33   buffer size ::: 13000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 10000\n",
      "epoch ::: 9   episode ::: 62.0   update step ::: 10000   time elapsed ::: 00:39:19\n",
      "critic loss avg ::: 46.28   min ::: 5.87    max ::: 551.92\n",
      "actor loss avg ::: 3.36   min ::: -4.92   max ::: 13.66\n",
      "q-vals avg ::: -3.42   min ::: -13.55   max ::: 4.81\n",
      "log_prob avg ::: -0.12   min ::: -0.41   max ::: 0.32\n",
      "env return avg ::: 82.91   buffer size ::: 14000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 11000\n",
      "epoch ::: 10   episode ::: 64.0   update step ::: 11000   time elapsed ::: 00:43:50\n",
      "critic loss avg ::: 44.05   min ::: 5.5    max ::: 498.48\n",
      "actor loss avg ::: 2.48   min ::: -7.79   max ::: 13.69\n",
      "q-vals avg ::: -2.57   min ::: -13.71   max ::: 7.68\n",
      "log_prob avg ::: -0.15   min ::: -0.51   max ::: 0.13\n",
      "env return avg ::: 10.77   buffer size ::: 15000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Early stop! Resetting...\n",
      "Update step: 1000\n",
      "epoch ::: 0   episode ::: 43.0   update step ::: 1000   time elapsed ::: 00:03:16\n",
      "critic loss avg ::: 230.41   min ::: 13.13    max ::: 888.99\n",
      "actor loss avg ::: 6.39   min ::: -0.41   max ::: 12.19\n",
      "q-vals avg ::: -6.84   min ::: -12.36   max ::: -0.3\n",
      "log_prob avg ::: -0.76   min ::: -1.38   max ::: -0.05\n",
      "env return avg ::: -103.96   buffer size ::: 5000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 2000\n",
      "epoch ::: 1   episode ::: 47.0   update step ::: 2000   time elapsed ::: 00:06:24\n",
      "critic loss avg ::: 106.49   min ::: 11.64    max ::: 311.76\n",
      "actor loss avg ::: 10.19   min ::: 5.93   max ::: 14.98\n",
      "q-vals avg ::: -10.25   min ::: -15.06   max ::: -6.01\n",
      "log_prob avg ::: -0.11   min ::: -0.51   max ::: 0.3\n",
      "env return avg ::: -215.96   buffer size ::: 6000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 3000\n",
      "epoch ::: 2   episode ::: 51.0   update step ::: 3000   time elapsed ::: 00:09:49\n",
      "critic loss avg ::: 74.47   min ::: 11.31    max ::: 249.26\n",
      "actor loss avg ::: 10.54   min ::: 5.37   max ::: 15.44\n",
      "q-vals avg ::: -10.53   min ::: -15.32   max ::: -5.44\n",
      "log_prob avg ::: 0.02   min ::: -0.29   max ::: 0.37\n",
      "env return avg ::: -150.24   buffer size ::: 7000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 4000\n",
      "epoch ::: 3   episode ::: 54.0   update step ::: 4000   time elapsed ::: 00:13:46\n",
      "critic loss avg ::: 62.49   min ::: 8.68    max ::: 188.36\n",
      "actor loss avg ::: 9.84   min ::: 4.54   max ::: 16.71\n",
      "q-vals avg ::: -9.86   min ::: -16.68   max ::: -4.57\n",
      "log_prob avg ::: -0.04   min ::: -0.38   max ::: 0.27\n",
      "env return avg ::: -203.45   buffer size ::: 8000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 5000\n",
      "epoch ::: 4   episode ::: 56.0   update step ::: 5000   time elapsed ::: 00:18:21\n",
      "critic loss avg ::: 51.59   min ::: 8.08    max ::: 155.61\n",
      "actor loss avg ::: 9.31   min ::: 2.24   max ::: 16.75\n",
      "q-vals avg ::: -9.36   min ::: -16.63   max ::: -2.34\n",
      "log_prob avg ::: -0.07   min ::: -0.43   max ::: 0.31\n",
      "env return avg ::: -77.53   buffer size ::: 9000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 6000\n",
      "epoch ::: 5   episode ::: 57.0   update step ::: 6000   time elapsed ::: 00:22:39\n",
      "critic loss avg ::: 43.03   min ::: 8.66    max ::: 181.06\n",
      "actor loss avg ::: 9.54   min ::: 3.26   max ::: 16.18\n",
      "q-vals avg ::: -9.59   min ::: -16.22   max ::: -3.34\n",
      "log_prob avg ::: -0.09   min ::: -0.38   max ::: 0.27\n",
      "env return avg ::: -26.16   buffer size ::: 10000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 7000\n",
      "epoch ::: 6   episode ::: 58.0   update step ::: 7000   time elapsed ::: 00:26:47\n",
      "critic loss avg ::: 35.68   min ::: 6.18    max ::: 127.67\n",
      "actor loss avg ::: 8.54   min ::: 1.01   max ::: 17.37\n",
      "q-vals avg ::: -8.61   min ::: -17.34   max ::: -1.11\n",
      "log_prob avg ::: -0.12   min ::: -0.41   max ::: 0.2\n",
      "env return avg ::: 6.06   buffer size ::: 11000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 8000\n",
      "epoch ::: 7   episode ::: 60.0   update step ::: 8000   time elapsed ::: 00:30:47\n",
      "critic loss avg ::: 31.46   min ::: 6.49    max ::: 132.16\n",
      "actor loss avg ::: 8.45   min ::: -0.81   max ::: 16.62\n",
      "q-vals avg ::: -8.52   min ::: -16.62   max ::: 0.67\n",
      "log_prob avg ::: -0.12   min ::: -0.41   max ::: 0.18\n",
      "env return avg ::: -4.53   buffer size ::: 12000\n",
      "current lr ::: 0.0003000000142492354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update step: 9000\n",
      "epoch ::: 8   episode ::: 63.0   update step ::: 9000   time elapsed ::: 00:35:03\n",
      "critic loss avg ::: 28.44   min ::: 5.84    max ::: 116.51\n",
      "actor loss avg ::: 6.66   min ::: -3.42   max ::: 15.38\n",
      "q-vals avg ::: -6.72   min ::: -15.36   max ::: 3.35\n",
      "log_prob avg ::: -0.1   min ::: -0.5   max ::: 0.2\n",
      "env return avg ::: -84.25   buffer size ::: 13000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 10000\n",
      "epoch ::: 9   episode ::: 64.0   update step ::: 10000   time elapsed ::: 00:39:01\n",
      "critic loss avg ::: 24.96   min ::: 6.27    max ::: 108.61\n",
      "actor loss avg ::: 5.1   min ::: -2.62   max ::: 12.81\n",
      "q-vals avg ::: -5.15   min ::: -12.81   max ::: 2.51\n",
      "log_prob avg ::: -0.09   min ::: -0.45   max ::: 0.28\n",
      "env return avg ::: -4.82   buffer size ::: 14000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 11000\n",
      "epoch ::: 10   episode ::: 65.0   update step ::: 11000   time elapsed ::: 00:42:30\n",
      "critic loss avg ::: 23.92   min ::: 6.04    max ::: 103.2\n",
      "actor loss avg ::: 5.63   min ::: -2.42   max ::: 14.48\n",
      "q-vals avg ::: -5.68   min ::: -14.54   max ::: 2.26\n",
      "log_prob avg ::: -0.09   min ::: -0.38   max ::: 0.18\n",
      "env return avg ::: -24.79   buffer size ::: 15000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 12000\n",
      "epoch ::: 11   episode ::: 66.0   update step ::: 12000   time elapsed ::: 00:47:08\n",
      "critic loss avg ::: 24.11   min ::: 5.7    max ::: 427.63\n",
      "actor loss avg ::: 5.3   min ::: -3.99   max ::: 16.72\n",
      "q-vals avg ::: -5.37   min ::: -16.73   max ::: 3.95\n",
      "log_prob avg ::: -0.11   min ::: -0.41   max ::: 0.22\n",
      "env return avg ::: 78.43   buffer size ::: 16000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 13000\n",
      "epoch ::: 12   episode ::: 67.0   update step ::: 13000   time elapsed ::: 00:51:49\n",
      "critic loss avg ::: 27.25   min ::: 5.92    max ::: 246.71\n",
      "actor loss avg ::: 5.19   min ::: -4.77   max ::: 15.9\n",
      "q-vals avg ::: -5.26   min ::: -15.91   max ::: 4.72\n",
      "log_prob avg ::: -0.12   min ::: -0.42   max ::: 0.19\n",
      "env return avg ::: 69.67   buffer size ::: 17000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 14000\n",
      "epoch ::: 13   episode ::: 69.0   update step ::: 14000   time elapsed ::: 00:56:23\n",
      "critic loss avg ::: 27.19   min ::: 6.21    max ::: 365.5\n",
      "actor loss avg ::: 5.42   min ::: -3.67   max ::: 13.58\n",
      "q-vals avg ::: -5.5   min ::: -13.64   max ::: 3.55\n",
      "log_prob avg ::: -0.13   min ::: -0.43   max ::: 0.27\n",
      "env return avg ::: 101.9   buffer size ::: 18000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 15000\n",
      "epoch ::: 14   episode ::: 70.0   update step ::: 15000   time elapsed ::: 01:00:43\n",
      "critic loss avg ::: 25.85   min ::: 6.15    max ::: 338.66\n",
      "actor loss avg ::: 4.41   min ::: -4.85   max ::: 15.68\n",
      "q-vals avg ::: -4.51   min ::: -15.71   max ::: 4.64\n",
      "log_prob avg ::: -0.16   min ::: -0.42   max ::: 0.08\n",
      "env return avg ::: 91.28   buffer size ::: 19000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 16000\n",
      "epoch ::: 15   episode ::: 71.0   update step ::: 16000   time elapsed ::: 01:05:43\n",
      "critic loss avg ::: 25.24   min ::: 5.39    max ::: 355.84\n",
      "actor loss avg ::: 3.36   min ::: -6.41   max ::: 12.32\n",
      "q-vals avg ::: -3.48   min ::: -12.38   max ::: 6.31\n",
      "log_prob avg ::: -0.19   min ::: -0.52   max ::: 0.13\n",
      "env return avg ::: 17.28   buffer size ::: 20000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 17000\n",
      "epoch ::: 16   episode ::: 72.0   update step ::: 17000   time elapsed ::: 01:10:08\n",
      "critic loss avg ::: 31.83   min ::: 5.38    max ::: 358.1\n",
      "actor loss avg ::: 1.69   min ::: -8.43   max ::: 12.72\n",
      "q-vals avg ::: -1.77   min ::: -12.81   max ::: 8.3\n",
      "log_prob avg ::: -0.13   min ::: -0.43   max ::: 0.13\n",
      "env return avg ::: 14.17   buffer size ::: 21000\n",
      "current lr ::: 0.0003000000142492354\n",
      "Update step: 18000\n",
      "epoch ::: 17   episode ::: 74.0   update step ::: 18000   time elapsed ::: 01:14:41\n",
      "critic loss avg ::: 31.29   min ::: 5.98    max ::: 349.21\n",
      "actor loss avg ::: 0.15   min ::: -8.29   max ::: 10.13\n",
      "q-vals avg ::: -0.23   min ::: -10.08   max ::: 8.2\n",
      "log_prob avg ::: -0.14   min ::: -0.4   max ::: 0.1\n",
      "env return avg ::: 25.35   buffer size ::: 22000\n",
      "current lr ::: 0.0003000000142492354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e3c3eea03f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;31m# backpropagate policy gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                     \u001b[0;31m# update policy network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/envs/ReAlly/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/envs/ReAlly/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/envs/ReAlly/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/envs/ReAlly/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1692\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.07/envs/ReAlly/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5525\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5527\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5528\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5529\u001b[0m         transpose_b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "restore_ckpt = None#saving_path.parent/\"model_08-03-2021_10-58-58_AM\"/'checkpoints'\n",
    "restart = True\n",
    "while restart:\n",
    "    now = time.strftime(\"%d-%m-%Y_%I-%M-%S_%p\", time.gmtime())\n",
    "    # initialize buffer\n",
    "    buffer = ReplayBuffer(buffer_size, env.observation_space.shape, env.action_space.shape)\n",
    "    # where to save the results\n",
    "    saving_path = Path(os.getcwd() + \"/../Homework/A3/progress_test/\" + f\"model_{now}\")\n",
    "    \n",
    "    # initialize actor\n",
    "    actor = ActorNetwork(**model_kwargs)\n",
    "    dummy = (np.zeros((1, *env.observation_space.shape)), np.zeros((1, *env.action_space.shape)))\n",
    "    actor(dummy[0])\n",
    "    \n",
    "    # initialize update and target double q networks\n",
    "    update_DoubleQNetwork = DoubleQNetwork(**qnet_kwargs)\n",
    "    update_DoubleQNetwork(*dummy)\n",
    "    # get seperate network for delayed DQN and copy weights\n",
    "    target_DoubleQNetwork = DoubleQNetwork(**qnet_kwargs)\n",
    "    target_DoubleQNetwork(*dummy)\n",
    "    target_DoubleQNetwork.set_weights(update_DoubleQNetwork.get_weights())\n",
    "    # disable gradients for target q-nets\n",
    "    for layer in target_DoubleQNetwork.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # init optimizer\n",
    "    critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    # map function for datasets\n",
    "    reshape_and_cast = lambda x: tf.cast(tf.reshape(x, (optim_batch_size, -1)), tf.float64)\n",
    "    \n",
    "    step, episode = tf.Variable(0, dtype=tf.int64), 0\n",
    "    \n",
    "    # init checkpoints\n",
    "    ckpt_q_update = tf.train.Checkpoint(model=update_DoubleQNetwork, optimizer=critic_optimizer, step=step)\n",
    "    ckpt_q_target = tf.train.Checkpoint(model=target_DoubleQNetwork, step=step)\n",
    "    ckpt_actor = tf.train.Checkpoint(model=actor, optimizer=actor_optimizer, step=step)\n",
    "\n",
    "    if restore_ckpt is not None:\n",
    "        ckpt_q_update.restore(tf.train.latest_checkpoint(restore_ckpt/'critic'/'update'))#.assert_consumed()\n",
    "        ckpt_q_target.restore(tf.train.latest_checkpoint(restore_ckpt/'critic'/'target')).assert_consumed()\n",
    "        ckpt_actor.restore(tf.train.latest_checkpoint(restore_ckpt/'actor'))#.assert_consumed()\n",
    "        start_steps = int(step)\n",
    "    \n",
    "    writer = tf.summary.create_file_writer(str(saving_path.parent/'logs'/f\"model_{now}\"))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    critic_losses, actor_losses, q_values, q_targets, log_probabilities = [], [], [], [], []\n",
    "    moving_mean, moving_reward_avg, moving_std, moving_reward_std = 0, 0, 0, 0\n",
    "    with writer.as_default(step=step):\n",
    "        for e in range(epochs):\n",
    "            # run agents\n",
    "            if e == 0:\n",
    "                cur_state = warmup(env, actor, buffer, warmup_steps)\n",
    "                episode += np.sum(buffer.data['done'])\n",
    "            for i in range(epoch_length):\n",
    "                for _ in range(train_every_n_step):\n",
    "                    action = actor.act(cur_state)['action']\n",
    "                    next_state, reward, done, info = env.step(action)\n",
    "                    buffer.store(cur_state, action, reward, next_state, done)\n",
    "                    cur_state = next_state\n",
    "                    if done:\n",
    "                        cur_state = env.reset()\n",
    "                        episode += 1\n",
    "\n",
    "                # update moving average\n",
    "                if normalize_inputs:\n",
    "                    state_action = np.concatenate((buffer.data['state'][:buffer.num_stored], buffer.data['action'][:buffer.num_stored]), axis=1)\n",
    "                    moving_mean = alpha_exp_avg if (e+i)!=0 else 1 * (np.mean(state_action, axis=0) - moving_mean)\n",
    "                    moving_std =  alpha_exp_avg if (e+i)!=0 else 1 * (np.std(state_action, axis=0) - moving_std)\n",
    "                    update_normalization(moving_mean, moving_std)\n",
    "\n",
    "                # sample data as tf datasets to optimize on from buffer\n",
    "                data_list = buffer.sample_list_of_datasets(\n",
    "                    optim_batch_size, \n",
    "                    sample_size,\n",
    "                    map_func=reshape_and_cast,\n",
    "                    prioritized_sampling=prioritized_sampling,\n",
    "                    prio_sampling_degree=prio_sampling_degree)\n",
    "\n",
    "                for i, (state, action, reward, next_state, done) in enumerate(zip(*data_list)):\n",
    "                    # sample next action from policy\n",
    "                    output = actor.act(next_state, return_log_prob=True)\n",
    "                    # get Q-values for next state of delayed network\n",
    "                    target_qs = target_DoubleQNetwork(next_state, output['action'])\n",
    "                    q_value_next = tf.reduce_min([target_qs['q1'], target_qs['q2']], axis=0)\n",
    "                    # compute target\n",
    "                    q_target = reward + (1 - done) * gamma * (q_value_next - alpha * output[\"log_probability\"])\n",
    "                    # record forward pass\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        qs_pred = update_DoubleQNetwork(state, action)\n",
    "                        loss_q1 = tf.reduce_mean(tf.keras.losses.MSE(q_target, qs_pred['q1']))\n",
    "                        loss_q2 = tf.reduce_mean(tf.keras.losses.MSE(q_target, qs_pred['q2']))\n",
    "\n",
    "                    # backpropagte loss\n",
    "                    gradients = tape.gradient([loss_q1, loss_q2], update_DoubleQNetwork.trainable_variables)\n",
    "                    # update q-network\n",
    "                    critic_optimizer.apply_gradients(zip(gradients, update_DoubleQNetwork.trainable_variables))\n",
    "                    \n",
    "                    # disable op tracking for update q-nets temporarily\n",
    "                    for layer in update_DoubleQNetwork.layers:\n",
    "                        layer.trainable = False\n",
    "\n",
    "                    # record forward pass\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        # sample action with updated policy\n",
    "                        output = actor.act(state, return_log_prob=True)\n",
    "                        qs = update_DoubleQNetwork(state, output['action'])\n",
    "                        q_value = tf.reduce_min([qs['q1'], qs['q2']], axis=0)\n",
    "                        actor_loss = tf.reduce_mean(alpha * output[\"log_probability\"] - q_value)\n",
    "\n",
    "                    q_targets.append(np.mean(q_target))\n",
    "                    q_values.append(np.mean(q_value))\n",
    "\n",
    "                    # backpropagate policy gradient\n",
    "                    gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "                    # update policy network\n",
    "                    if not np.isnan(np.mean([np.mean(grad) for grad in gradients])):\n",
    "                        actor_optimizer.apply_gradients(zip(gradients, actor.trainable_variables))\n",
    "                    else:\n",
    "                        \"Skipping nan gradient.\"\n",
    "                        \n",
    "                    # reenable op tracking for update q-nets\n",
    "                    for layer in update_DoubleQNetwork.layers:\n",
    "                        layer.trainable = True\n",
    "\n",
    "                    critic_losses.append(float(loss_q1 + loss_q2))\n",
    "                    actor_losses.append(float(actor_loss))\n",
    "                    log_probabilities.append(np.mean(output[\"log_probability\"]))\n",
    "\n",
    "                    # update delayed network\n",
    "                    old = [polyak * weight for weight in target_DoubleQNetwork.get_weights()]\n",
    "                    new = [(1 - polyak) * weight for weight in update_DoubleQNetwork.get_weights()]\n",
    "                    target_DoubleQNetwork.set_weights([old_w + new_w for old_w, new_w in zip(old, new)])\n",
    "\n",
    "                    step.assign_add(1)\n",
    "                if int(step) % (log_every_n_step) == 0:\n",
    "                    tf.summary.scalar('actor/actor_loss', np.mean(actor_losses[-log_every_n_step:]))\n",
    "                    tf.summary.scalar('critic/critic_loss', np.mean(critic_losses[-log_every_n_step:]))\n",
    "                    tf.summary.scalar('actor/log_prob', np.mean(log_probabilities[-log_every_n_step:]))\n",
    "                    tf.summary.scalar('critic/q_vals', np.mean(q_values[-log_every_n_step:]))\n",
    "                    tf.summary.scalar('critic/q_targets', np.mean(q_targets[-log_every_n_step:]))\n",
    "\n",
    "            print(f\"Update step: {int(step)}\")\n",
    "\n",
    "            if e % test_every == 0:\n",
    "                # print progress\n",
    "                returns = test(env_id, actor, test_episodes=6, render=False)\n",
    "                returns.extend(test(env_id, actor, test_episodes=4, render=True))\n",
    "                mean_return = np.mean(returns)\n",
    "                \n",
    "                tf.summary.scalar('metrics/test_return', mean_return)\n",
    "                print(\n",
    "                    f\"epoch ::: {e}   episode ::: {episode}   update step ::: {int(step)}   time elapsed ::: {time.strftime('%H:%M:%S', time.gmtime((time.time() - start_time)))}\",   \n",
    "                    f\"critic loss avg ::: {np.round(np.mean(critic_losses), 2)}   min ::: {np.round(np.min(critic_losses), 2)}    max ::: {np.round(np.nanmax(critic_losses), 2)}\",   \n",
    "                    f\"actor loss avg ::: {np.round(np.mean(actor_losses), 2)}   min ::: {np.round(np.min(actor_losses), 2)}   max ::: {np.round(np.nanmax(actor_losses), 2)}\", \n",
    "                    f\"q-vals avg ::: {np.round(np.mean(q_values), 2)}   min ::: {np.round(np.nanmin(q_values), 2)}   max ::: {np.round(np.nanmax(q_values), 2)}\", \n",
    "                    f\"log_prob avg ::: {np.round(np.mean(log_probabilities), 2)}   min ::: {np.round(np.min(log_probabilities), 2)}   max ::: {np.round(np.nanmax(log_probabilities), 2)}\",\n",
    "                    f\"env return avg ::: {np.round(mean_return, 2)}   buffer size ::: {buffer.num_stored}\",\n",
    "                    f\"current lr ::: {np.round(actor_optimizer._decayed_lr('float32').numpy(), 5)}\",\n",
    "                    sep='\\n'\n",
    "                )\n",
    "                if mean_return >= 200:\n",
    "                    restart = False\n",
    "\n",
    "            # early stopping metric\n",
    "            if (e % (test_every * 5) == 0) and (e != 0):\n",
    "                now = time.strftime(\"%d-%m-%Y_%I-%M-%S_%p\", time.gmtime())\n",
    "                ckpt_q_update.save(saving_path/'checkpoints'/'critic'/'update'/f\"model_{e}_{now}\")\n",
    "                ckpt_q_target.save(saving_path/'checkpoints'/'critic'/'target'/f\"model_{e}_{now}\")\n",
    "                ckpt_actor.save(saving_path/'checkpoints'/'actor'/f\"model_{e}_{now}\")\n",
    "                if (mean_return < old_return_avg) and (e < 25):\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    print(\"Early stop! Resetting...\")\n",
    "                    break\n",
    "\n",
    "            if (e % (test_every * 5) == 0) or (e == 0):\n",
    "                old_return_avg = mean_return\n",
    "\n",
    "\n",
    "            if e % test_every == 0:\n",
    "                critic_losses, actor_losses, q_values, q_targets, log_probabilities = [], [], [], [], []\n",
    "            \n",
    "            writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-palace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
